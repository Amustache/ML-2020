{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load implementations.py\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from tqdm import tqdm\n",
    "\n",
    "def mini_batch(y, tx, size, num):\n",
    "    N = len(y)\n",
    "    # Get random indices\n",
    "    batch_ind = np.random.permutation(np.arange(N))\n",
    "    print(\"batch_ind\")\n",
    "    print(batch_ind)\n",
    "    batch_ind = batch_ind[:size]\n",
    "    print(\"batch_ind\")\n",
    "    print(batch_ind)\n",
    "    for i in batch_ind:\n",
    "        print(i)\n",
    "        batch_y = y[i]\n",
    "    print(y[batch_ind])\n",
    "    print(batch_y)\n",
    "    print(tx[batch_ind])\n",
    "\n",
    "def mae_loss(y, tx, w):\n",
    "    \"\"\" Compute the MAE loss. \"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    return np.sum(np.abs(e))/(2*len(y))\n",
    "\n",
    "\n",
    "def mse_loss(y, tx, w):\n",
    "    \"\"\" Compute the MSE loss. \"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    return np.sum(np.square(e))/(2*len(y))\n",
    "\n",
    "\n",
    "\"\"\" Requested ML methods. \"\"\"\n",
    "def least_square_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\" Linear regression using gradient descent. \"\"\"\n",
    "    w = initial_w\n",
    "    for i in tqdm(range(max_iters)):\n",
    "        # Compute error vector\n",
    "        e = y-tx.dot(w)\n",
    "        # Compute gradient\n",
    "        grad = (-1/len(y)) * tx.transpose().dot(e)\n",
    "        # Compute w(t+1)\n",
    "        w = w - gamma*grad\n",
    "        # Compute loss\n",
    "        loss = mse_loss(y, tx, w)\n",
    "    return w, loss\n",
    "\n",
    "def least_square_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\" Linear regression using stochastic gradient descent. \"\"\"\n",
    "    return w, loss\n",
    "\n",
    "def least_squares(y, tx):\n",
    "    \"\"\" Least squares using normal equations. \"\"\"\n",
    "    w = inv(tx.transpose().dot(tx)).dot(tx.transpose()).dot(y)\n",
    "    loss = mse_loss(y, tx, w)\n",
    "    return w, loss\n",
    "\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\" Ridge regression using normal equations. \"\"\"\n",
    "    N = len(y)\n",
    "    w = np.inv(np.add(tx.transpose().dot(tx), 2*N*lambda_ * np.identity(N))).dot(tx.transpose()).dot(y)\n",
    "    loss = mse_loss(y, tx, w)\n",
    "    return w, loss\n",
    "\n",
    "def logistic_function(z):\n",
    "    \"\"\" The logistic function sigma. \"\"\"\n",
    "    return np.exp(z)/(1+np.exp(z))\n",
    "\n",
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\" Logistic regression using gradient descent or SGD. \"\"\"\n",
    "    for i in range(max_iters):\n",
    "        loss = np.sum(np.log(1 + np.exp(tx.dot(w))) - y * (tx.dot(w)))\n",
    "        grad = tx.transpose().dot(logistic_function(tx.dot(x))-y)\n",
    "    w = w - gamma*grad\n",
    "    return w, loss\n",
    "\n",
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    \"\"\" Regularized logistic regression using gradient descent or SGD. \"\"\"\n",
    "    return w, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load parser.py\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "def read_csv(data_path):\n",
    "    \"\"\" Return ids, data and predictions from csv file given a data_path. \"\"\"\n",
    "    data = np.genfromtxt(data_path, delimiter=\",\", skip_header=1)\n",
    "    # Predicitons are string so must extract them separately to avoid NaN results\n",
    "    y_str = np.genfromtxt(data_path, delimiter=\",\", skip_header=1, dtype=str, usecols=1)\n",
    "    \n",
    "    # Extract ids, x and y\n",
    "    ids = data[:, 0].astype(np.int)\n",
    "    x = data[:, 2:]\n",
    "\n",
    "    # Convert string prediction to binary 0 or 1\n",
    "    y = np.ones(len(y_str))\n",
    "    y[np.where(y_str=='b')] = 0\n",
    "\n",
    "    return y, x, ids \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, tx_train, ids_train = read_csv(\"data/train.csv\")\n",
    "y_test, tx_test, ids_test = read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(tx_train.shape[1])\n",
    "initial_w[0] = 1\n",
    "max_iters = 100\n",
    "gamma = 0.00000001\n",
    "\n",
    "least_square_GD(y_train, tx_train, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
